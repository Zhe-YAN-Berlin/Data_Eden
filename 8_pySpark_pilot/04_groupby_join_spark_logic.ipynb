{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d177101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/08 19:06:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/08 19:06:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/03/08 19:06:30 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd7b36c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green = spark.read.parquet('download_data/data/pq/green/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48ea9b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green.createOrReplaceTempView ('green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55e6b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_revenue = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    date_trunc('hour', lpep_pickup_datetime) AS hour, \n",
    "    PULocationID AS zone,\n",
    "\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    green\n",
    "WHERE\n",
    "    lpep_pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f37a5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------------------+--------------+\n",
      "|               hour|zone|            amount|number_records|\n",
      "+-------------------+----+------------------+--------------+\n",
      "|2020-01-28 19:00:00| 134|193.61000000000007|            17|\n",
      "|2020-01-22 19:00:00|  65| 657.0300000000001|            41|\n",
      "|2020-01-27 08:00:00|  17|             85.56|             4|\n",
      "|2020-01-02 09:00:00|  66|229.39999999999998|            12|\n",
      "|2020-01-02 12:00:00|  89|310.28000000000003|            14|\n",
      "|2020-01-07 12:00:00|  66|             179.5|             9|\n",
      "|2020-01-03 08:00:00| 223|165.90000000000003|             9|\n",
      "|2020-01-17 10:00:00|  41| 638.2699999999999|            49|\n",
      "|2020-01-02 11:00:00|  26|             198.6|             7|\n",
      "|2020-01-12 20:00:00| 247|36.900000000000006|             3|\n",
      "|2020-01-12 15:00:00|  10|             75.92|             1|\n",
      "|2020-01-24 04:00:00| 129|148.89000000000001|            13|\n",
      "|2020-01-04 20:00:00|  25| 369.5700000000001|            23|\n",
      "|2020-01-22 10:00:00|  74|1179.0599999999988|            83|\n",
      "|2020-01-13 18:00:00|  92|207.02000000000004|            11|\n",
      "|2020-01-19 11:00:00| 218|            181.16|             5|\n",
      "|2020-01-26 12:00:00|  35|257.03000000000003|             8|\n",
      "|2020-01-30 06:00:00|  75|            282.63|            19|\n",
      "|2020-01-04 22:00:00|  83|            298.16|            15|\n",
      "|2020-01-06 21:00:00| 129|288.02000000000004|            14|\n",
      "+-------------------+----+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green_revenue.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e7d91ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green_revenue \\\n",
    "    .repartition(20) \\\n",
    "    .write.parquet('download_data/data/report/revenue/green', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8372bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_revenue = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    date_trunc('hour', lpep_pickup_datetime) AS hour, \n",
    "    PULocationID AS zone,\n",
    "\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    green\n",
    "WHERE\n",
    "    lpep_pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3435a656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green_revenue \\\n",
    "    .repartition(20) \\\n",
    "    .write.parquet('download_data/data/report/revenue/green', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bda8558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_revenue = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    date_trunc('hour', lpep_pickup_datetime) AS hour, \n",
    "    PULocationID AS zone,\n",
    "\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    green\n",
    "WHERE\n",
    "    lpep_pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2\n",
    "ORDER BY\n",
    "    1, 2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f86a98ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green_revenue \\\n",
    "    .repartition(20) \\\n",
    "    .write.parquet('download_data/data/report/revenue/green', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e221931",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = spark.read.parquet('download_data/data/pq/yellow/*/*')\n",
    "df_yellow.registerTempTable('yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e3383b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow_revenue = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    date_trunc('hour', tpep_pickup_datetime) AS hour, \n",
    "    PULocationID AS zone,\n",
    "\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    yellow\n",
    "WHERE\n",
    "    tpep_pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddb8b065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_yellow_revenue \\\n",
    "    .repartition(20) \\\n",
    "    .write.parquet('download_data/data/report/revenue/yellow', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abf9b75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------------------+--------------+\n",
      "|               hour|zone|            amount|number_records|\n",
      "+-------------------+----+------------------+--------------+\n",
      "|2020-02-02 01:00:00| 264|             32.46|             3|\n",
      "|2020-01-18 11:00:00| 223|             86.38|             7|\n",
      "|2020-01-15 09:00:00| 126|             58.22|             2|\n",
      "|2020-01-03 10:00:00| 167|114.42999999999999|             6|\n",
      "|2020-02-15 11:00:00|  29|             20.74|             1|\n",
      "|2020-01-05 09:00:00|  82|104.27999999999999|            10|\n",
      "|2020-02-13 07:00:00|   7|            152.18|            11|\n",
      "|2020-01-19 22:00:00|  74| 609.5299999999999|            48|\n",
      "|2020-02-11 05:00:00| 102|             35.04|             1|\n",
      "|2020-01-18 21:00:00| 259|              36.0|             1|\n",
      "|2020-02-07 10:00:00|  75| 969.5999999999993|            64|\n",
      "|2020-01-16 23:00:00|  29|              61.8|             1|\n",
      "|2020-02-25 06:00:00| 146|             16.05|             1|\n",
      "|2020-02-16 04:00:00|  56|              23.8|             1|\n",
      "|2020-02-19 17:00:00| 101|             31.79|             1|\n",
      "|2020-02-15 09:00:00| 135|             72.15|             2|\n",
      "|2020-02-10 16:00:00| 244| 959.9699999999997|            36|\n",
      "|2020-01-26 20:00:00|  91|              40.0|             1|\n",
      "|2020-01-20 07:00:00|  78|             56.64|             1|\n",
      "|2020-01-13 18:00:00|  29|              36.0|             1|\n",
      "+-------------------+----+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------------+----+------------------+--------------+\n",
      "|               hour|zone|            amount|number_records|\n",
      "+-------------------+----+------------------+--------------+\n",
      "|2020-02-02 01:00:00| 264|             32.46|             3|\n",
      "|2020-01-18 11:00:00| 223|             86.38|             7|\n",
      "|2020-01-15 09:00:00| 126|             58.22|             2|\n",
      "|2020-01-03 10:00:00| 167|114.42999999999999|             6|\n",
      "|2020-02-15 11:00:00|  29|             20.74|             1|\n",
      "|2020-01-05 09:00:00|  82|104.27999999999999|            10|\n",
      "|2020-02-13 07:00:00|   7|            152.18|            11|\n",
      "|2020-01-19 22:00:00|  74| 609.5299999999999|            48|\n",
      "|2020-02-11 05:00:00| 102|             35.04|             1|\n",
      "|2020-01-18 21:00:00| 259|              36.0|             1|\n",
      "|2020-02-07 10:00:00|  75| 969.5999999999993|            64|\n",
      "|2020-01-16 23:00:00|  29|              61.8|             1|\n",
      "|2020-02-25 06:00:00| 146|             16.05|             1|\n",
      "|2020-02-16 04:00:00|  56|              23.8|             1|\n",
      "|2020-02-19 17:00:00| 101|             31.79|             1|\n",
      "|2020-02-15 09:00:00| 135|             72.15|             2|\n",
      "|2020-02-10 16:00:00| 244| 959.9699999999997|            36|\n",
      "|2020-01-26 20:00:00|  91|              40.0|             1|\n",
      "|2020-01-20 07:00:00|  78|             56.64|             1|\n",
      "|2020-01-13 18:00:00|  29|              36.0|             1|\n",
      "+-------------------+----+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green_revenue = spark.read.parquet('download_data/data/report/revenue/green')\n",
    "df_green_revenue.show()\n",
    "df_yellow_revenue = spark.read.parquet('download_data/data/report/revenue/yellow')\n",
    "df_green_revenue.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9e40e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_revenue_tmp = df_green_revenue \\\n",
    "    .withColumnRenamed('amount', 'green_amount') \\\n",
    "    .withColumnRenamed('number_records', 'green_number_records')\n",
    "\n",
    "df_yellow_revenue_tmp = df_yellow_revenue \\\n",
    "    .withColumnRenamed('amount', 'yellow_amount') \\\n",
    "    .withColumnRenamed('number_records', 'yellow_number_records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "277351eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join = df_green_revenue_tmp.join(df_yellow_revenue_tmp, on=['hour', 'zone'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7dd5f061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_join.write.parquet('download_data/data/report/revenue/total', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a617424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hour: timestamp, zone: int, green_amount: double, green_number_records: bigint, yellow_amount: double, yellow_number_records: bigint]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea5e4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c01fd509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 80:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------------------+--------------------+------------------+---------------------+\n",
      "|               hour|zone|      green_amount|green_number_records|     yellow_amount|yellow_number_records|\n",
      "+-------------------+----+------------------+--------------------+------------------+---------------------+\n",
      "|2020-01-01 00:00:00|   3|              null|                null|              25.0|                    1|\n",
      "|2020-01-01 00:00:00|   4|              null|                null|1004.3000000000002|                   57|\n",
      "|2020-01-01 00:00:00|   7| 769.7299999999996|                  45| 455.1700000000001|                   38|\n",
      "|2020-01-01 00:00:00|  12|              null|                null|             107.0|                    6|\n",
      "|2020-01-01 00:00:00|  37|            175.67|                   6|161.60999999999999|                    7|\n",
      "|2020-01-01 00:00:00|  40|168.97999999999996|                   8|             89.97|                    5|\n",
      "|2020-01-01 00:00:00|  45|              null|                null| 732.4800000000002|                   42|\n",
      "|2020-01-01 00:00:00|  47|              13.3|                   1|               8.3|                    1|\n",
      "|2020-01-01 00:00:00|  51|              17.8|                   2|              31.0|                    1|\n",
      "|2020-01-01 00:00:00|  62|             15.95|                   1|             61.43|                    1|\n",
      "|2020-01-01 00:00:00|  68|              null|                null| 7825.070000000012|                  396|\n",
      "|2020-01-01 00:00:00|  73|              null|                null|              17.3|                    1|\n",
      "|2020-01-01 00:00:00|  74|317.09000000000015|                  24| 586.2100000000002|                   47|\n",
      "|2020-01-01 00:00:00|  77| 75.99000000000001|                   2|            134.24|                    3|\n",
      "|2020-01-01 00:00:00|  80|364.32000000000005|                  13|             371.4|                   20|\n",
      "|2020-01-01 00:00:00|  83|              94.1|                   7|               9.8|                    1|\n",
      "|2020-01-01 00:00:00|  88|              null|                null| 823.8000000000002|                   36|\n",
      "|2020-01-01 00:00:00|  89|              11.3|                   1|             48.16|                    2|\n",
      "|2020-01-01 00:00:00|  94| 64.24000000000001|                   3|              13.3|                    1|\n",
      "|2020-01-01 00:00:00|  95|202.20000000000002|                  15|               6.3|                    1|\n",
      "+-------------------+----+------------------+--------------------+------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 83:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1724b2b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/home/datatalks_jan/Data_Eden/8_pySpark_pilot/zones.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_zones \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzones/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/spark/spark-3.4.2-bin-hadoop3/python/pyspark/sql/readwriter.py:531\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    520\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    522\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    523\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    529\u001b[0m )\n\u001b[0;32m--> 531\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mparquet(_to_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc, paths)))\n",
      "File \u001b[0;32m~/spark/spark-3.4.2-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/spark/spark-3.4.2-bin-hadoop3/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/home/datatalks_jan/Data_Eden/8_pySpark_pilot/zones."
     ]
    }
   ],
   "source": [
    "df_zones = spark.read.parquet('zones/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521462c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
