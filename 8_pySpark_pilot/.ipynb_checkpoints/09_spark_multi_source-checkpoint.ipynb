{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "126f726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import gdown\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62bfafe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1vcb_HBWsOSKW4XxhLfRpGlLzBLwHlGWJ\n",
      "From (redirected): https://drive.google.com/uc?id=1vcb_HBWsOSKW4XxhLfRpGlLzBLwHlGWJ&confirm=t&uuid=0ec89e14-43e6-41f6-bfeb-271900c9f504\n",
      "To: /home/datatalks_jan/Data_Eden/8_pySpark_pilot/source_data/multi_source_data/multi_source_demo.zip\n",
      "100%|█████████████████████████████████████████████████████████████| 154M/154M [00:00<00:00, 187MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'source_data/multi_source_data/multi_source_demo.zip'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_url = 'https://drive.google.com/uc?id=1vcb_HBWsOSKW4XxhLfRpGlLzBLwHlGWJ'\n",
    "output_path = 'source_data/multi_source_data'\n",
    "output_file = os.path.join(output_path, 'multi_source_demo.zip') \n",
    "\n",
    "gdown.download(source_url, output_file, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfa431c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  source_data/multi_source_data/multi_source_demo.zip\n",
      "  inflating: source_data/multi_source_data/distribution_centers.csv  \n",
      "  inflating: source_data/multi_source_data/events.csv  \n",
      "  inflating: source_data/multi_source_data/inventory_items.csv  \n",
      "  inflating: source_data/multi_source_data/order_items.csv  \n",
      "  inflating: source_data/multi_source_data/orders.csv  \n",
      "  inflating: source_data/multi_source_data/products.csv  \n",
      "  inflating: source_data/multi_source_data/readme.txt  \n",
      "  inflating: source_data/multi_source_data/users.csv  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(f'unzip -o {output_file} -d {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9cf5ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/03 10:50:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName('conrad_test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fdba772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_1_users = spark.read.option(\"header\", \"true\").csv(f'{output_path}/users.csv')\n",
    "df_1_distri_centers = spark.read.option(\"header\", \"true\").csv(f'{output_path}/distribution_centers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e87d7666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, acos, cos, sin, radians, atan2, sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54b80489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    distance = 6371 * c \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b8e896c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.haversine_distance(lat1, lon1, lat2, lon2)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"haversine_distance\", haversine_distance) # for spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b570f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_users.createOrReplaceTempView(\"users\")\n",
    "df_1_distri_centers.createOrReplaceTempView(\"distribution_centers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eee79df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_sql_query = \"\"\"\n",
    "    SELECT u.id, u.age, u.country, u.state,\n",
    "           dc.id AS distribution_center_id,\n",
    "           haversine_distance(u.latitude, u.longitude, dc.latitude, dc.longitude) AS distance\n",
    "    FROM users u\n",
    "    CROSS JOIN distribution_centers dc\n",
    "    ORDER BY u.id, distance\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c822b256",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_centers_df = spark.sql(df_1_sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85450b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
